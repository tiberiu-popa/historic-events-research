\chapter{Implementation}
\label{chapter:implementation}

\begin{figure}
\centering
\includegraphics[max size={\textwidth}{\textheight}]{modules}
\caption{Modules diagram}
\label{fig:modules}
\end{figure}

The implementation revolves around a few highly decoupled modules, who communicate with each other through data stored on the disk, as shown in \autoref{fig:modules}. The data is stored in various formats, which I will describe now. First, there is the Google Books Ngram Corpus, made freely available by Google, stored in comma-separated values (CSV) format. Second, I need to process a lot of data fast, so the obvious choice is to create a Binary Ngram format. This format requires the use of not one but three files: a main file that stores metadata for each possible n-gram, such as offsets in the next two files at which to find further information; a words file that stores all the n-grams as one long string; a time file that stores all words' time series. Third, the historically relevant documents are stored in CSV format. Fourth and finally, the topic data is generated by an external tool and is stored in a combination of CSV and text files.

The modules deal with transforming the data from and to various formats. The Binary Ngram Module is tasked with reading and writing the files used in the Binary Ngram format. Also, it includes a trivial CSV parser for the Google Books Ngram Corpus and is fully implemented in C/C++. The Historical Relevance Module implements all the algorithms discussed in \labelindexref{Section}{sec:historical-relevance-algorithms}, relying on the Binary Ngram Module to access the data as fast as possible, and its main purpose is to create the historically relevant documents. Again, this part is also implemented in C/C++. The Topic Modeling Module uses an external tool, the Stanford Topic Modeling Toolbox \cite{stanfordtmt}, called using a Scala script, to create a topic model from the historically relevant documents. The Processing Module helps understand the data obtained at various stages of the process. For example, it transforms the output of the Historical Relevance Module to a graphical form, allowing for comparison with the original time series. Furthermore, it analyzes the output of the Stanford Topic Modeling Toolbox to create a cleaner representation of which topics correspond to which time intervals. This final module is written in Python.
