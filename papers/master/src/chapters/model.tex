\chapter{Historic Events Model}
\label{chapter:hist-ev-model}

\section{Corpus, Smoothing and Historical Relevance}
\label{sec:corpus-smoothing-relevance}

The full Google Books Ngram Corpus has been made freely available by Google. In this paper, only statistics for 1-grams are used. More specifically, the analysis shall be based on the time series it provides for each word, mapping the year of publishing to the frequency of the n-gram in books written in that year. In order to deny the negative effect that the natural increase in books published each year may have on the relevance of the time series, the model's input shall instead be the time series of normalized frequencies. The time spanned by the series is between $1500$ and $2008$.

Although much effort has been put into ensuring the quality of the Google Books Ngram Corpus, it still contains noise from sources such as erroneous date of publication. In order to mitigate this effect, the simple solution is to smooth the series. More formally, say that the time series for a given word $w_i$ is denoted by $\left\{ s_{i, t} \right\}_{t = 1500}^{2008}$. Then, the smoothed series with window $k$ is given by

\begin{align}
\label{eq:smooth-series}
u_{k} \left( i, t \right) = \frac{\sum_{d = -k}^{k} s_{i, t + d}}{2k + 1}, \, \forall t \in \overline{1500 + d, 2008 - d}.
\end{align}

From now on, all references to $\left\{ s_{i, t} \right\}$ shall instead refer to the associated smoothed series $u_k$ defined in equation \eqref{eq:smooth-series} above, for a value of $k$ that was empirically set to $2$.

The data provided by the Google Books Ngram Corpus is immense (only the 1-grams have a size of about $30$ GB), so trying to identify historic events using all of the raw data seems to be an intractable problem. In order to solve this issue, I have opted to perform a preliminary transformation on the data, with the purpose of reducing the size to a more manageable order of magnitude. Of course, the transformation should retain the property of historical relevance of an n-gram in a given year, so the subsequent analysis can correctly model the influence of historic events on texts. Regarding the structure of the time series, it should transform a time series of normalized frequencies (real numbers between $0$ and $1$) to a time series reflecting historical relevance on a discrete scale, for example the integers in $\left[ 0, 10 \right]$, where $0$ would reflect lack of historical relevance of the n-gram in that year and $10$ would reflect an n-gram that basically defines that year. Due to the implicit localized nature of historic events, any given n-gram should attain historical relevance only within a small time frame. Thus, most of the entries in the reduced time series should be zero, leading to a highly sparse representation. Therefore, not only is this transformation useful from a practical point of view (it drastically reduces the computational resources required for the analysis), but it also has theoretical arguments that it maps well to the task of historic events identification.

The exact scale used for the reduced time series is not fixed beforehand and it is worth discussing its influence. Thus, representing historical relevance on a binary scale leads to an extremely sparse model that can be analyzed very fast, but that will lead to mediocre results. Increasing the number of degrees of relevance will lead to better and better representations and consequently to results of higher quality. There is an unavoidable trade-off to be made here, namely the one between the time necessary to run model and its quality, so the scale will be practically limited by the amount of computational resources available for the subsequent analysis of the reduced time series.

\section{Bursty Features}
\label{sec:bursty-features}

On an intuitive level, the historical relevance of a given n-gram seems intimately connected to unusual bursts in the frequency of that n-gram. As it turns out, this related problem has been studied before, albeit in a slightly different setting. Thus, in \cite{Kleinberg:2002:BHS:775047.775061}, the author considered text streams, i.e. documents that are chronologically ordered, and posed the problem of identifying for each feature the periods in which it presents bursts of activity. It was the first formal framework developed for detecting these bursts and provided invaluable insight into analyzing the content of any given document stream.

Furthermore, Kleinberg has identified in \cite{Kleinberg:2002:BHS:775047.775061} that there are two types of text streams to consider. On the one hand, in cases such as e-mail messages the precise order in which they arrive is pivotal for understanding their content, so they can be considered fast-paced. On the other hand, scientific papers belonging to some research field do not exhibit the same property. One reason identified by the author is that papers are published only in batches, thus providing only a limited amount of information for comparing the times at which the papers where written. Moreover, in my opinion, it is actually more important that it takes at least one year (usually even more) before a paper can influence further research, so that the specific order of the papers doesn't matter anyway. However, the order on a larger scale (years) is the key to discovering trends in scientific research, so this kind of document streams can be called slow-paced. Even if the documents wouldn't come naturally in batches, we'd still have to split them artificially, since otherwise the analysis would be extremely noisy.

The Google Books Ngram Corpus clearly belongs to the second category, as it is derived from books written during the course of history. Books written within a short time span have a high probability of being correlated, so their precise ordering is unlikely to matter. Again, the Ngram Corpus is also naturally divided into batches, as it only provides statistics for all books written in a year and not for every book.

\section{Existing Algorithms for Bursty Feature Detection}
\label{sec:bf-existing-algorithms}

\subsection{Kleinberg's Weighted Automaton Model}
\label{subsec:bf-kleinberg}

As mentioned before, there are two flavours to the algorithm introduced in \cite{Kleinberg:2002:BHS:775047.775061}. Despite the fact that only the second version of this model (the batch version) is relevant to the current research, the first one (the continuous version) will also be presented for clarity of explanation.

\subsubsection{The Continuous Model}

This version assumes that we are given $m$ documents $d_1, d_2, \ldots, d_m$ and that for each document $i \in \overline{1,m}$ there is an associated timestamp $t_i$. Furthermore, the timestamps are sorted in increasing order $t_1 < t_2 < \ldots < t_m$. Note that it is also assumed that no two documents have the same timestamp, for reasons that will become apparent later.

Now, given any feature $g$ (for example, whether a document contains the word "war"), one selects from all documents only those that have the feature $g$: $d_{g,0}, \ldots, d_{g,n}$. Instead of working with the raw timestamps, a sequence of positive gaps must be constructed:

\begin{align}
\label{eq:positive-gaps}
x_i &= t_{g,i} - t_{g,i - 1}, \, \forall i \in \overline{1,n}.
\end{align}

At the core of the model lies the idea of generating the sequence of gaps using a probability distribution. For this purpose one can use the family of exponential density functions

\begin{align}
\label{eq:exponential-density}
f(x) &= \alpha \exp \left( -\alpha x \right),
\end{align}

parameterized by $\alpha > 0$. This has an expected value of $\frac1{\alpha}$, which can be thought of as the rate at which documents containing some given feature are generated.

The obvious drawback to this simple approach is that all the $x_i$'s are generated independently. A more realistic model should consider the fact that documents contain some given feature $g$ at varying rates throughout the complete timeline. Obviously, for this approach to work, one also needs to select a discrete set of $\alpha$'s from which to pick the document rates.

Going back to the documents $d_{g,0}, \ldots, d_{g,n}$, let's say that they span a time period of $\displaystyle T = \sum_{i=1}^{n} x_i$. If we supposed that the documents' timestaps were equally-spaced, the rate of arrival would equal $\frac{n}{T}$. In order to generate it, we can simply choose to use the parameter $\alpha_0 = \frac{T}{n}$, whose expected value is indeed $\frac{1}{\alpha_0} = \frac{n}{T}$. This is used as the base rate (i.e. the slowest rate) for the model. The other possible rates are computed using the formula $\alpha_i = \alpha_0 s^i$, where $i > 0$ is an integer and $s > 1$ is the ratio of the geometric progression $\left\{ \alpha_i \right\}_{i=0}^{\infty}$.

The crux of Kleinberg's solution consists of introducing an automaton with the states $q_i$, $i \geq 0$, corresponding to exponential density functions parameterized by $\alpha_i$. Thus, much like in a hidden Markov model, the gap between two documents is determined by one of the hidden states of the automaton. However, unlike a hidden Markov model, instead of assigning transition probabilities between every two states $i, j$, the author preferred to use costs $\tau \left( i, j \right)$. The main requirements are:

\begin{itemize}
\item $\tau \left( i, j \right) = 0$, if $i \geq j$,
\item $\tau \left( i, j \right) \propto (j - i)$, otherwise.
\end{itemize}

In other words, the automaton can freely drop to a lower state (equivalent to a lower document rate) without incurring any cost, but the cost of going to a higher state is proportional to the number of states skipped. The coefficient of proportionality in the second case was chosen to be $\gamma \ln n$, where $\gamma > 0$ is yet another parameter. Thus, $\tau \left( i, j \right) = \left( j - i \right) \gamma \ln n$, so that $\gamma$ controls the degree to which switching states is desirable. The constructed infinite automaton is usually denoted by $A_{s, \gamma}^{\star}$.

Now, given a sequence of positive gaps $\left\{ x_i \right\}_{i=1}^{n}$, the cost corresponding to any possible latent state sequence $\left\{ q_{i_j} \right\}_{j=1}^{n}$ is given by

\begin{align}
\label{eq:positive-gaps}
c \left( q | x \right) &= \sum_{t=0}^{n-1} \tau \left( i_t, i_{t + 1} \right) - \sum_{t=1}^{n} \ln f_{i_t} \left( x_t \right),
\end{align}

where $f_{i_t}$ is the exponential density function $f$ with parameter $\alpha_{i_t}$. As it can be noticed, the cost function includes another term besides the transition costs. In fact, it has an inverse dependence on the probability that the observation $x_t$ was generated by the latent state $q_{i_t}$. Thus, the problem of finding bursty features has been reduced to computing a latent sequence that minimizes a certain cost function.

Unfortunately, the automaton has an infinite number of states and that seems to render the problem very difficult. However, one of the core theorems in \cite{Kleinberg:2002:BHS:775047.775061} claims that the automaton can be reduced to a finite number of $k$ states, $q_0, q_1, \ldots, q_{k-1}$, and the minimum latent sequence stays the same. Of course, $k$ depends on the gaps sequence, but in practice has proved to be quite small (i.e. $k = 25$ is enough for most applications).

Once the problem is reduced to a finite automaton, it is rather easy to solve using dynamic programming. More precisely, one can just adapt Viterbi's algorithm introduced in \cite{Viterbi:1054010} to use costs instead of probabilities.

\subsubsection{The Discrete Model}

In the discrete version, we are now presented with batches of documents, instead of a single document at a time. Suppose that there are $n$ batches, such that batch $b_t$ contains $d_t$ documents. Also, the total number of documents is denoted by $\displaystyle D = \sum_{t=1}^n d_t$.

Given some feature $g$, let $r_t$ be the number of documents in batch $b_t$ that contain it (the number of relevant documents). Furthermore, the total number of relevant documents is denoted by $\displaystyle R = \sum_{t=1}^n r_t$. It is then easy to deduce that the expected ratio of relevant document is $\frac{R}{D}$.

Following the continuous model, the author defined an automaton with states $q_i, \, i \geq 0$, such that the associated expected ratio of relevant documents equals $p_i = \frac{R}{D} s^i$, where $s > 1$ is a scaling parameter. The proposed notation for this automaton is $B_{s, \gamma}^{\star}$. However, the expected ratio has to satisfy $p_i \leq 1$, so $i$ can only take finitely many values. Thus, the automaton is again finite. It remains to define the costs associated with the automaton.

Firstly, the state $q_i$ generates relevant and non-relevant documents according to the binomial distribution with parameter $p_i$. Thus, when the automaton finds itself in state $q_i$ and tries to generate batch $b_t$, i.e. $d_t$ documents out of which only $r_t$ are relevant, the cost incurred is

\begin{align}
\label{eq:binomial-distribution}
\sigma \left( i, d_t, r_t \right) = -\ln \left[ \binom{d_t}{r_t} p_i^{r_t} \left( 1 - p_i \right)^{d_t - r_t} \right].
\end{align}

Secondly, one also needs to model the transition costs between states. In this case, the formula used for the continuous model also works here:

\begin{align}
\label{eq:discrete-transition}
\tau \left( i, j \right) &=
	\begin{cases}
		(j - i) \gamma \ln n & \textrm{if} \, i < j, \\
		0 & \textrm{otherwise}.
	\end{cases}
\end{align}

Moving forward, the procedure for determining the minimum state sequence remains the same.

\subsubsection{Burst Weights}

Now that a method of detecting bursts has been presented, there is a dire need for assigning weights to bursts, corresponding to their perceived intensity. In \cite{Kleinberg:2002:BHS:775047.775061}, only the particular case of $B_{s, \gamma}^2$ (an automaton with only $2$ states) is studied. Thus, bursty activity can be taken to be synonymous with the state $q_1$, while bursts are defined as maximal intervals of time $\left[ t_1, t_2 \right]$ in which the state is constantly $q_1$.

The proposed method for computing the weight of a burst is as the decrease in cost due to using $q_1$ on the bursty interval instead of $q_0$. More formally,

\begin{align}
\label{eq:automaton-burst-weights}
w &= \sum_{t=t_1}^{t_2} \left( \sigma \left( 0, r_t, d_t \right) - \sigma \left( 1, r_t, d_t \right) \right).
\end{align}

Besides the method proposed in the original paper, there is a viable alternative for assigning weights in the case we're interested in, the discrete model. Thus, suppose we are presented with a full automaton $B_{s, \gamma}^{k}$. Instead of assigning burst weights to whole intervals, we shall instead assign a burst weight to each year individually. The weight for a year whose latent state is $q_i$ satisfies $w \propto i$. The constant of proportionality is dependent on the feature under scrutiny and is necessary so that weights are comparable between different features.

\subsection{Numerical Discrepancy}
\label{subsec:bf-discrepancy}

\subsubsection{Application to Search}

Ever since bursty features were first introduced, a growing number of applications have been found for them. One of the more important uses is the search of documents streams. Thus, in \cite{Lappas:2009:BSD:1557019.1557075}, the authors propose a framework for searching timestamped collections such as newspaper articles. Their work is divided into three main components.

Firstly, they propose a new strategy for indexing documents and evaluating queries. Instead of relying on standard measures such term frequency inverted document frequency (or tf-idf), the score of a term with respect to a document is actually computed as a combination between the term frequency (the classical part) and the burstiness of the term at the time of the writing of the article (the novel part). The score of a query with respect to a document is then naturally defined as the sum of the document scores of each term in the query. The problem of searching a query is reduced to find the top $k$ documents according to the order imposed by the query's document score.

Secondly, another type of problem is identified: searching for time intervals in which the terms of the query were most bursty. Thus, the score of a term with respect to an interval is defined as the burstiness of that term on a super-segment of the given interval. After that, the score of a query with respect to an interval can be computed just as the sum of the interval scores of its terms. Again, the search problem is solved by returning the top $k$ intervals, as ordered by the query's interval score.

Lastly, a novel method for computing burstiness is proposed. Among the advantages, the most important are the existence of a mathematical theory behind it, discrepancy theory, and the fact that the resulting method is parameter free.

\subsubsection{Burstiness Model}

Suppose we are given a set of random points $\mathcal{P}$ inside the unit interval $\mathcal{U} = \left[ 0, 1 \right]$. Pick some interval $I \subseteq \mathcal{U}$. If the points are truly randomly distributed, then the expected ratio of points that lie inside $I$ should equal the measure of the interval, $len \left( I \right)$. Numerical discrepancy of the set $\mathcal{P}$ with respect to an interval $I$ is defined as the amount by which the ratio differs from the expected value \cite{Dobkin:1996:CMB:233502.233508}:

\begin{align}
\label{eq:numerical-discrepancy}
D_{\mathcal{P}} \left( I \right) &= \left| len \left( I \right) - \frac{\left| \mathcal{P} \cap I \right|}{\left| \mathcal{P} \right|} \right|.
\end{align}

Following \cite{Lappas:2009:BSD:1557019.1557075}, this concept can also be applied to the time series corresponding to the frequency of a given term $t$. Suppose the documents' timestamps span the period $\left[ 0, m \right]$. Let's also fix a time interval $\left[ l, r \right] \subseteq \left[ 1, m \right]$ and denote by $\left\{ s_{t,i} \right\}_{i=l}^{r}$ the frequency series for term $t$ on that interval. The definition of numerical discrepancy can also be adapted for this case by scaling the interval $\left[ 0, m \right]$ to the unit interval $\mathcal{U}$, such that $\left[ l, r \right]$ also gets mapped to $I_{l,r} \subseteq \mathcal{U}$. The length of $I$ is easily computed as:

\begin{align}
\label{eq:nd-interval-length}
len \left( I_{l,r} \right) &= \frac{len \left( \mathcal{U} \right)}{len \left( \left[ 0, m \right] \right)} \cdot len \left( \left[ l, r \right] \right) \\
	&= \frac{r - l}{m}. \notag
\end{align}

Furthermore, the number of points that fall in a given interval can be computed as the sum of the frequencies on that interval: $\displaystyle \sum_{i=l}^{r} s_{t,i}$. Thus, the ratio of points falling into the query interval is quite simply computed using the formula:

\begin{align}
\label{eq:nd-point-ratio}
\frac{\left| \mathcal{P} \cap I_{l,r} \right|}{\left| \mathcal{P} \right|} &= \frac{\sum_{i=l}^{r} s_{t,i}}{\sum_{i=1}^{m} s_{t,i}}.
\end{align}

Therefore, the numerical discrepancy can finally be defined as:

\begin{align}
\label{eq:nd-time-series-absolute}
\mathcal{D}_{P} \left( I_{l,r} \right) &= \left| \frac{r - l}{m} - \frac{\sum_{i=l}^{r} s_{t,i}}{\sum_{i=1}^{m} s_{t,i}} \right|.
\end{align}

Thus, numerical discrepancy measures the amount by which the actual behaviour of a time series on a given interval differs from the expected average behaviour. However, this difference is considered only in \textbf{absolute value}. In \cite{Lappas:2009:BSD:1557019.1557075}, it is argued that one is actually interested only in time intervals where features perform significantly above their average behaviour. Thus, the definition chosen for burstiness is:

\begin{align}
\label{eq:nd-time-series-bursty}
\mathcal{B}_{t} \left( \left[ l, r \right] \right) &= \frac{\sum_{i=l}^{r} s_{t,i}}{\sum_{i=1}^{m} s_{t,i}} - \frac{r - l}{m}.
\end{align}

In order to efficiently compute this quantity, one can express the sum in terms of an additional series:

\begin{align}
\label{eq:nd-mini-series}
y_{t,i} &= \frac{s_{t,i}}{\sum_{j=1}^{m} s_{t,j}} - \frac{1}{m}, \\
\label{eq:nd-mini-series-sum}
\mathcal{B}_{t} \left( \left[ l, r \right] \right) &= \sum_{i=l}^{r} y_{t,i}.
\end{align}

The problem of finding all of the feature's bursts is reduced to finding intervals that maximize the associated burstiness function. According to \autoref{eq:nd-mini-series-sum}, this is equivalent to finding intervals that maximize segment sums of a related series. This problem, known as All Maximal Segments Problem, was solved in \cite{Ruzzo99alinear} using a very efficient linear time algorithm.

\section{Historical Relevance Algorithms}
\label{sec:historical-relevance-algorithms}
\input{src/chapters/submodel/historical-relevance-algorithms}

\section{Historically Relevant Documents and Topic Models}
\label{sec:hist-relevant-doc}

Now that several ways of assigning historical relevance to an n-gram have been developed, the problem of identifying historical events can finally be tackled. For each year $t \in \left[ 1500, 2008 \right]$, we build its historically relevant document $d_t$ as follows: we include each $w_i$ (that satisfies $r_{i, t} > 0$) exactly $r_{i, t}$ times.

However, the sheer size of the documents make it intractable to analyze them by hand. Topic models seem most suitable for solving this problem, as they are designed for discovering the hidden thematic structure in document collections \cite{Blei:2012:PTM:2133806.2133826}. The specific algorithm used in this paper is the Latent Dirichlet Allocation (LDA) \cite{Blei:2003:LDA:944919.944937}. In this generative model, each topic is represented as a multinomial distribution over the words. To generate a document of given length, one first selects a topic mixture from a Dirichlet distribution over all topics. Then, for each word, one selects a topic from the previous topic mixture, which is used for sampling the word.

Upon applying Latent Dirichlet Allocation to the historically relevant documents, ideally the topics found by the algorithm should be types of historic events and the topic distribution for any given year should represent the underlying historic events going on in that year. However, what ends up happening is that recurrent themes for consecutive years are grouped into a topic. While this still needs the intervention of a human to interpret the results, the data that must be analyzed is significantly smaller in size. Thus, for every period in time, we will get a set of keywords, some of which will point to historic events, while others may point to cultural trends.
