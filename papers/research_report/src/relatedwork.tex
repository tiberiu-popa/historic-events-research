% vim: set tw=78 sts=2 sw=2 ts=8 aw et ai:

In \cite{Michel14012011}, Michel et al. have laid the foundation of culturomics, i.e. the quantitative analysis of culture. Thus, using a corpus of over 5 million books, i.e. about $4 \%$ of all the books ever published, they have managed to computationally investigate cultural trends. This meant that now social sciences have a complementary approach available to the traditional analysis usually employed.

The essential ingredient of the study was of course the large corpus used. Without it, it would have been very difficult to select an unbiased sample of texts that yielded significant results. The exact same corpus, Google Books Ngram Corpus, shall be used in this paper, so I'll go into greater detail about its structure. Thus, over 15 million books have been digitized for the Google Books project using optical character recognition (or OCR). The main languages used in the books are English, French, Spanish, German, Chinese, Russian and Hebrew. The resulting corpus was then filtered to increase its quality in two regards. First of all, they used a clever algorithm to estimate the accuracy of the OCR for any given book and set a threshold on the quality, generally around $50 - 80 \%$, depending on the language. Secondly, the metadata for each book was collected from various sources, but many times conflicts would appear. Therefore, they also removed books that had low accuracy of metadata referring to the date of publication and to the language. In the end, about 5 million books remained as a basis for the Ngram Corpus.

Instead of providing access to the raw books, due to copyright restrictions, the Google Books Ngram Corpus actually consists of n-gram statistics for sequences of up to 5 words. Thus, for each n-gram, the following information is available for any given year: the number of times the n-gram has appeared in books written in that year, the number of pages in which it appeared and the number of volumes in which it appeared. Since the number of books written each year is constantly increasing, one can also derive more informative measures from this data. One way to do this would to be to normalize the statistics. For example, one can divide the first one by the total number of n-grams that appeared in books written in that year. The last measure presented is also used by Google Books Ngram Viewer \cite{ngramviewer}, a web application for visualizing the time series for one or more n-grams.

The applications of culturomics are numerous, the most obvious of which would be in linguistics. It has been found that the English lexicon has about 1 million words, while the largest dictionary lists only $350,000$ of them. Also, one can use the data to study the evolution of grammar, such as the transition of verbs from irregular to regular or vice versa.

However, the most interesting applications lie in the social sciences. For example, by studying the time series of various inventions, one can analyze the rate at which technology is adopted into widespread use. Other examples are closely related to historic events. Firstly, Nazi censorship has left visible marks on the published books and censored individuals can be easily identified by comparing their n-grams during the Nazi regime with those in the adjacent periods of time. Secondly, one can deduce from the plot associated with influenza when the most devastating flu pandemics have occurred: they correspond to the peaks in the time series. The same idea works in the case of the American Civil War, using the graphs of "the North" and "the South".

This kind of analyses can also be applied to more recent corpora, such as news archives. In \cite{leetaru11culturomics}, Leetaru has used data from the Summary of World Broadcasts to analyze events such as the Arab Spring and ethnic conflicts such as the one that occurred in Serbia in the 1990s. Another more spectacular application is determining the location of Bin Laden's hideout by analyzing cities that appear in the same article with his name, the result being off by less than 200 kilometers. Although not relevant specifically to the current research, it is a further proof of the utility of culturomics in analyzing historic events.

This paper will focus on peaks of n-gram time series, such as those of influenza, as previously mentioned. However, instead of examining particular cases, I shall analyze a significant portion of the time series by running a peak detection algorithm. The resulting data shall then be used for characterizing the underlying historic events.
