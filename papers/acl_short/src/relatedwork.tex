% vim: set tw=78 sts=2 sw=2 ts=8 aw et ai:

In \newcite{Michel14012011}, the authors have laid the foundation of culturomics, i.e. the quantitative analysis of culture. Thus, using a corpus of over 5 million books, i.e. about $4 \%$ of all the books ever published, they have managed to computationally investigate cultural trends.

The Google Books Ngram Corpus consists of n-gram statistics for sequences of up to 5 words. Thus, for each n-gram, the following information is available for any given year: the number of times the n-gram has appeared in books written in that year, the number of pages in which it appeared and the number of volumes in which it appeared. Furthermore, it can be very helpful to normalize the statistics. For example, one can divide the first one by the total number of n-grams that appeared in books written in that year.

The most interesting applications of culturomics lie in the social sciences. For example, one can analyze various historic events. Firstly, Nazi censorship has left visible marks on the published books and censored individuals can be easily identified by comparing their n-grams during the Nazi regime with those in the adjacent periods of time. Secondly, one can deduce from the plot associated with influenza when the most devastating flu pandemics have occurred: they correspond to the peaks in the time series.

This kind of analyses can also be applied to more recent corpora, such as news archives. In \newcite{leetaru11culturomics}, Leetaru has used data from the Summary of World Broadcasts to analyze events such as the Arab Spring and ethnic conflicts such as the one that occurred in Serbia in the 1990s. Although not relevant specifically to the current research, it is a further proof of the utility of culturomics in analyzing historic events.

\newcite{Hall:2008:SHI:1613715.1613763} have studied a related question, but with a narrower focus: the history of ideas in the field of Computational Linguistics. By analyzing the ACL Anthology using topic models, they revealed historical trends such as the rise of probabilistic models and the decline of plan-based dialogue over the last 25 years. Another study that influenced the present paper is \newcite{Wijaya:2011:USC:2064448.2064475}. There, the authors use the Google Books Ngram Corpus to analyze the semantic evolution of words over centuries. The core idea is to consider words that co-occur with a given word and then to apply topic modeling to them, the end result being that topics correspond to historical meanings of a word. The particular model used is Topics over Time (TOT), introduced in \newcite{Wang:2006:TOT:1150402.1150450}. Its advantage lies in the fact that it explicitly considers time as a factor influencing the structure of a document, leading to more accurate trajectories of topics (or, in this case, meanings of a word) over time.
