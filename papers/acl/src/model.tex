% vim: set tw=78 sts=2 sw=2 ts=8 aw et ai:

\subsection{Corpus, Smoothing and Historical Relevance}

The analysis shall be based on the Google Books Ngram Corpus and the time series it provides for each word, mapping the year of publishing to the normalized frequency of the n-gram in books written in that year. The time spanned by the series is between $1500$ and $2008$. Let us denote the time series for a given word $w_i$ by $\left\{ s_{i, t} \right\}_{t = 1500}^{2008}$. In order to mitigate the effect of noise on the data, we instead use the smoothed series with window size $k$, given by:

\begin{align}
\label{eq:smooth-series}
u_{k} \left( i, t \right) = \frac{\sum_{d = -k}^{k} s_{i, t + d}}{2k + 1}.
\end{align}

The value of this window size was empirically chosen to be $k = 2$.

The data provided by the Google Books Ngram Corpus is immense (about 30 GB), so trying to identify historic events using all of the raw data seems to be an intractable problem. Thus, a better idea is to model the historical relevance of an n-gram in a given year on a discrete scale, for example the integers in $\left[ 0, 10 \right]$, where $0$ would reflect the n-gram's lack of importance and $10$ would reflect a year-defining n-gram. Due to the implicit localized nature of historic events, any given n-gram should attain historical relevance only within a small time frame. Thus, most of the entries in the reduced time series should be $0$, leading to a highly sparse representation. Therefore, not only is this transformation useful from a practical point of view (it drastically reduces the computational resources required for the analysis), but it also has theoretical arguments that it maps well to the task of historic events identification.

\subsection{Historical Relevance Algorithms}
\label{sec:historical-relevance-algorithms}
\input{src/model/historical-relevance-algorithms}

\subsection{Historically Relevant Documents and Topic Models}

For each year $t \in \left[ 1500, 2008 \right]$, we build its historically relevant document $d_t$ as follows: we include each $w_i$ (that satisfies $r_{i, t} > 0$) exactly $r_{i, t}$ times. Topic models seem most suitable for automatically analyzing the data, as they are designed for discovering the hidden thematic structure in document collections \newcite{Blei:2012:PTM:2133806.2133826}. The specific algorithm used in this paper is the Latent Dirichlet Allocation (LDA) \newcite{Blei:2003:LDA:944919.944937}. Upon applying it to the historically relevant documents, ideally the topics found by the algorithm should represent the types of historic events, while the topic distribution for any given year should represent the underlying historic events. However, what ends up happening is that recurrent themes for consecutive years are grouped into a topic. While this still needs the intervention of a human to interpret the keywords pointing either to historic events or to cultural trends, the data that must be analyzed is significantly smaller in size.
